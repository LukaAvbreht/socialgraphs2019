{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib, urllib2, json\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def request_current_page(my_title):\n",
    "    \"\"\"\n",
    "    Find the content and time of last revision of a wikipedia page. \n",
    "    Input\n",
    "    ****************\n",
    "    title: (string)\n",
    "    \n",
    "    Output\n",
    "    **************\n",
    "    (string), time of last revision\n",
    "    (string), content of the page   \n",
    "    \n",
    "    \"\"\"\n",
    "    url = 'http://en.wikipedia.org/w/api.php/?'\n",
    "    values = {'action' : 'query',\n",
    "              'prop' : 'revisions',\n",
    "              'titles' : my_title,\n",
    "              'rvprop' : 'timestamp|content',\n",
    "              'format' : 'json'}\n",
    "\n",
    "    #generate query\n",
    "    data = urllib.urlencode(values)\n",
    "    req = url+data\n",
    "\n",
    "    #query the api\n",
    "    response = urllib2.urlopen(req)\n",
    "    json_page = json.loads(response.read())\n",
    "\n",
    "    first_revision = json_page['query']['pages'].values()[0]['revisions'][0]\n",
    "    revision_content = first_revision[u'*']\n",
    "    \n",
    "    #the date of creation\n",
    "    revised_on = first_revision[u'timestamp']   \n",
    "    return revised_on, revision_content.encode('utf-8')\n",
    "\n",
    "def parse_table(page,n1,n2, find_link_name = True):\n",
    "    \"\"\"\n",
    "    Parse wikipedia table of the senate and house of representatives.\n",
    "    Input\n",
    "    ****************\n",
    "    page: (string), page name\n",
    "    n1: (int), number of the table in order of appearance in the page\n",
    "    n2: (int), column where the name appears\n",
    "    \n",
    "    Output\n",
    "    **************\n",
    "    (pandas dataframe), content, party, state\n",
    "    \n",
    "    \"\"\"\n",
    "    r = requests.get(page)\n",
    "    sp = BeautifulSoup(r.content, \"html.parser\")\n",
    "    parsed_table = sp.find_all('table')[n1] \n",
    "    data = [[td.a['href'].replace('/wiki/','') if n==n2 and td.find('a') else \n",
    "             ''.join(td.stripped_strings)\n",
    "             for n,td in enumerate(row.find_all(['th','td']))]\n",
    "            for row in parsed_table.find_all('tr')]\n",
    "\n",
    "    df = pd.DataFrame(data[1:])\n",
    "\n",
    "    return df\n",
    "\n",
    "def find_current_name(my_title):\n",
    "    \"\"\"\n",
    "    Find current name of a given page.\n",
    "    \n",
    "    Input\n",
    "    ***********\n",
    "    my_title (string): page name\n",
    "    Output\n",
    "    *******************\n",
    "    (string): current page name\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    my_title = urllib.unquote(str(my_title))\n",
    "    print my_title,\n",
    "    timestamp, revision_content = request_current_page(my_title)\n",
    "    if 'REDIRECT' in revision_content:\n",
    "        new_title = re.findall(r'\\[\\[(.*?)\\]\\]',revision_content)[0]\n",
    "        print new_title\n",
    "        return new_title.replace(' ','_')\n",
    "    else:\n",
    "        return my_title.replace(' ','_')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define dictionaries for states and parties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dictionary between states and their abbreviation\n",
    "abb_page = \"https://en.wikipedia.org/wiki/List_of_U.S._state_abbreviations\"\n",
    "abbrev = pd.read_html(abb_page,encoding='utf-8',attrs={\"class\":\"wikitable\"}) [0][12:].groupby(3)[0].first().to_dict()\n",
    "\n",
    "#Dictionary betwen parties and their abbreviation\n",
    "parties = {'D':'Democratic','R':'Republican','I':'Indipendent'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desired pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/\"\n",
    "pages = ['List_of_members_of_the_United_States_House_of_Representatives_in_the_113th_Congress_by_seniority',\n",
    "         'List_of_members_of_the_United_States_House_of_Representatives_in_the_114th_Congress_by_seniority',\n",
    "         'List_of_members_of_the_United_States_House_of_Representatives_in_the_115th_Congress_by_seniority']\n",
    "\n",
    "number_of_the_table = [0,0,0]\n",
    "name_column = [1,1,1]\n",
    "\n",
    "pages = [url+i for i in pages]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run code (takes some time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [parse_table(page,n1,n2) for page,n1, n2 in zip(pages, number_of_the_table, name_column)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust columns and write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfs[0]['State'] = dfs[0]['District'].apply(lambda x: x.split('-')[0])\n",
    "dfs[0]['State'] = dfs[0]['State'].map(abbrev)\n",
    "dfs[0] = dfs[0][['Representative','Party','State']].rename(columns = {'Representative':'WikiPageName'})\n",
    "dfs[0]['WikiPageName'] = dfs[0]['WikiPageName'].apply(lambda x:find_current_name(x))\n",
    "dfs[0]['Party'] = dfs[0]['Party'].map(parties)\n",
    "dfs[0].to_csv('./data/'+names[0], encoding = 'utf-8',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfs[1]['State'] = dfs[1]['District'].apply(lambda x: ' '.join(x.split(u'\\xa0')[:-1]))\n",
    "dfs[1] = dfs[1][['Representative','Party','State']].rename(columns = {'Representative':'WikiPageName'})\n",
    "dfs[1]['Party'] = dfs[1]['Party'].map(parties)\n",
    "dfs[1]['WikiPageName'] = dfs[1]['WikiPageName'].apply(lambda x:find_current_name(x))\n",
    "dfs[1].to_csv('./data/'+names[1], encoding = 'utf-8',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfs[2]['State'] = dfs[2]['District'].apply(lambda x: ' '.join(x.split(u'\\xa0')[:-1]))\n",
    "dfs[2] = dfs[2][['Representative','Party','State']].rename(columns = {'Representative':'WikiPageName'})\n",
    "dfs[2]['WikiPageName'] = dfs[2]['WikiPageName'].apply(lambda x:find_current_name(x))\n",
    "dfs[2]['Party'] = dfs[2]['Party'].map(parties)\n",
    "dfs[2].to_csv('./data/'+names[2], encoding = 'utf-8',index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
